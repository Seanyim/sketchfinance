import streamlit as st
import pandas as pd
from PIL import Image
import os
import sys
import json
import torch
import gc
from datetime import datetime

# Aggressive memory management for CUDA
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
from streamlit_paste_button import paste_image_button

# Add project root to path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import importlib.util
from backend.app.services.ocr_service import OCRService
from backend.app.services.ai_enhancer import AIEnhancerService
from backend.app.services.ai_local_service import AIEnhancerLocal
from backend.app.models.finance_model import init_db, SessionLocal, FinancialRecordModel
from backend.app.repositories.finance_repo import FinanceRepository
from shared.schemas.finance import FinancialRecordCreate

# Helper: Load Config
def load_financial_metrics(config_path):
    spec = importlib.util.spec_from_file_location("config", config_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module.FINANCIAL_METRICS

# Initialize database
init_db()

st.set_page_config(page_title="SketchFinance - è´¢åŠ¡æ•°æ®è¯†å½•", layout="wide")

# Sidebar - Settings & Config
st.sidebar.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")
ollama_url = st.sidebar.text_input("Ollama URL", value="http://localhost:11434")
ollama_model = st.sidebar.text_input("Ollama Model", value="llama3")

st.sidebar.divider()
st.sidebar.header("ğŸ“ æŒ‡æ ‡é…ç½®")
default_config_path = os.path.join(os.getcwd(), "backend", "config", "config.py")
uploaded_config = st.sidebar.file_uploader("ä¸Šä¼ è‡ªå®šä¹‰ config.py", type=['py'])

if uploaded_config:
    temp_config_path = "temp_config.py"
    with open(temp_config_path, "wb") as f:
        f.write(uploaded_config.getbuffer())
    FINANCIAL_METRICS = load_financial_metrics(temp_config_path)
    st.sidebar.success("å·²åŠ è½½è‡ªå®šä¹‰é…ç½®")
else:
    FINANCIAL_METRICS = load_financial_metrics(default_config_path)

# Sidebar - Optimization Settings
st.sidebar.header("âš™ï¸ æ€§èƒ½è®¾ç½®")
gpu_ocr_enabled = st.sidebar.toggle("âš¡ OCR GPU åŠ é€Ÿ", value=False, help="å¦‚æœå¼€å¯æœ¬åœ° AI æ¨¡å‹ï¼Œå»ºè®®å…³é—­æ­¤é¡¹ä»¥èŠ‚çœæ˜¾å­˜")

# Initialize Services with Safety
if 'ocr_service' not in st.session_state:
    try:
        st.session_state.ocr_service = OCRService(gpu=gpu_ocr_enabled)
    except Exception as e:
        st.sidebar.warning(f"GPU OCR åˆå§‹åŒ–å¤±è´¥: {e}ã€‚å°†å›é€€åˆ° CPU æ¨¡å¼ã€‚")
        st.session_state.ocr_service = OCRService(gpu=False)

st.sidebar.info("æç¤ºï¼šå¦‚æœæ˜¾å¡æ˜¾å­˜(VRAM)ä¸è¶³å¯¼è‡´æŠ¥é”™ï¼Œè¯·ä¿æŒä¸Šé¢å¼€å…³å¤„äº**å…³é—­**çŠ¶æ€ã€‚")

if 'ai_enhancer' not in st.session_state:
    st.session_state.ai_enhancer = AIEnhancerService(ollama_url=ollama_url, model=ollama_model)
else:
    st.session_state.ai_enhancer.ollama_url = f"{ollama_url}/api/generate"
    st.session_state.ai_enhancer.model = ollama_model

# Clean memory periodically
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()

if 'db' not in st.session_state:
    st.session_state.db = SessionLocal()
    st.session_state.repo = FinanceRepository(st.session_state.db)

if 'auto_disclosure_date' not in st.session_state:
    st.session_state.auto_disclosure_date = ""

# Sidebar - Samples and History
st.sidebar.header("ğŸ“ å‚è€ƒä¸å†å²")
sample_dir = os.path.join(os.getcwd(), "samples")
if not os.path.exists(sample_dir):
    os.makedirs(sample_dir)

samples = [f for f in os.listdir(sample_dir) if f.endswith(('.png', '.jpg', '.jpeg', '.db'))]
selected_sample = st.sidebar.selectbox("é€‰æ‹©å‚è€ƒæ–‡ä»¶", ["æ— "] + samples)

if selected_sample != "æ— ":
    st.sidebar.info(f"æ­£åœ¨æŸ¥çœ‹: {selected_sample}")

# Helper: Filter metrics by category
def get_metrics_by_category(category):
    return [m for m in FINANCIAL_METRICS if m.get('category') == category]

# Main Layout: Two columns
col_up, col_res = st.columns([1, 1])

with col_up:
    # Category & Disclosure Date
    st.header("1. é…ç½®åŸºç¡€ä¿¡æ¯")
    cats = globals().get('CATEGORY_ORDER', ["å…³é”®æŒ‡æ ‡", "åˆ©æ¶¦è¡¨", "èµ„äº§è´Ÿå€ºè¡¨", "ç°é‡‘æµé‡è¡¨"])
    selected_category = st.selectbox("è¯·é€‰æ‹©è¦è¯†åˆ«çš„æŠ¥è¡¨ç±»å‹", cats)
    current_metrics = get_metrics_by_category(selected_category)
    
    # Disclosure date will be extracted automatically
    if st.session_state.auto_disclosure_date:
        st.success(f"ğŸ“… è¯†åˆ«åˆ°æŠ¥è¡¨æˆªæ­¢æ—¥: {st.session_state.auto_disclosure_date}")

    # Multi-Image Upload
    st.header("2. ä¸Šä¼ /ç²˜è´´æˆªå›¾æ¨¡å—")
    st.info("æç¤ºï¼šæ‚¨å¯ä»¥ç›´æ¥ç‚¹å‡»æŒ‰é’®å¹¶ä½¿ç”¨ Ctrl+V ç²˜è´´æˆªå›¾")
    
    use_local_ai = st.toggle("ğŸš€ ä½¿ç”¨æœ¬åœ° Transformers æ¨¡å‹æ·±åº¦çº é”™", value=True, help="å¯ç”¨åå°†ä½¿ç”¨æœ¬åœ° 0.5B æ¨¡å‹å¯¹è¯†åˆ«ç»“æœè¿›è¡Œè¯­ä¹‰å¾®è°ƒ")

    col_p, col_m, col_v = st.columns(3)
    with col_p:
        st.subheader("ğŸ“… å­£åº¦")
        upload_p = st.file_uploader("æ–‡ä»¶", type=["png", "jpg", "jpeg"], key="up_p")
        paste_p = paste_image_button("ğŸ“‹ ç²˜è´´å­£åº¦", key="p_p")
        img_p = upload_p if upload_p else (paste_p.image_data if paste_p.image_data else None)
        if img_p: st.image(img_p)
    with col_m:
        st.subheader("ğŸ“Š ç§‘ç›®")
        upload_m = st.file_uploader("æ–‡ä»¶", type=["png", "jpg", "jpeg"], key="up_m")
        paste_m = paste_image_button("ğŸ“‹ ç²˜è´´ç§‘ç›®", key="p_m")
        img_m = upload_m if upload_m else (paste_m.image_data if paste_m.image_data else None)
        if img_m: st.image(img_m)
    with col_v:
        st.subheader("ğŸ’° æ•°æ®")
        upload_v = st.file_uploader("æ–‡ä»¶", type=["png", "jpg", "jpeg"], key="up_v")
        paste_v = paste_image_button("ğŸ“‹ ç²˜è´´æ•°æ®", key="p_v")
        img_v = upload_v if upload_v else (paste_v.image_data if paste_v.image_data else None)
        if img_v: st.image(img_v)

    if st.button("ğŸš€ å¼€å§‹å¤šå›¾æ™ºèƒ½è¯†åˆ«", use_container_width=True):
        if not (img_p and img_m and img_v):
            st.warning("è¯·ä¸Šä¼ å®Œæ•´çš„ä¸‰ä¸ªéƒ¨åˆ†æˆªå›¾ã€‚")
        else:
            with st.spinner(f"æ­£åœ¨æ·±åº¦è§£æ {selected_category}..."):
                # Save temp
                paths = []
                for img, n in [(img_p, "p"), (img_m, "m"), (img_v, "v")]:
                    path = f"temp_{n}.png"
                    if hasattr(img, 'save'): # PIL Image from paste
                        img.save(path)
                    else: # Bytes/UploadedFile
                        with Image.open(img) as o_img:
                            o_img.save(path)
                    paths.append(path)
                
                if torch.cuda.is_available():
                    gc.collect()
                    torch.cuda.empty_cache()
                
                # Perform Multi-Image OCR
                try:
                    parsed_data, extracted_date = st.session_state.ocr_service.parse_multi_image(
                        paths[0], paths[1], paths[2], current_metrics
                    )
                except Exception as e:
                    st.error(f"OCR è¯†åˆ«å¤±è´¥: {e}. å»ºè®®å…³é—­ä¾§è¾¹æ  'OCR GPU åŠ é€Ÿ' åé‡è¯•ã€‚")
                    parsed_data, extracted_date = None, None

                if extracted_date:
                    st.session_state.auto_disclosure_date = extracted_date
                
                # Boost with Local AI if enabled
                if use_local_ai and parsed_data:
                    if 'ai_local' not in st.session_state:
                        with st.status("ğŸ§  æ­£åœ¨åˆå§‹åŒ–æœ¬åœ° AI æ¨¡å‹ (å¯èƒ½è§¦å‘ GPU å†…å­˜è­¦å‘Š)..."):
                            try:
                                gc.collect()
                                if torch.cuda.is_available(): torch.cuda.empty_cache()
                                st.session_state.ai_local = AIEnhancerLocal()
                            except Exception as e:
                                st.error(f"æœ¬åœ° AI æ¨¡å‹åŠ è½½å¤±è´¥: {e}ã€‚å°†ç¦ç”¨ AI å¢å¼ºã€‚")
                                use_local_ai = False
                    
                    with st.spinner("ğŸ¤– æœ¬åœ° AI æ­£åœ¨æ·±åº¦æ ¡éªŒè¯†åˆ«ç»“æœ..."):
                        # Convert parsed data back to text for AI to see context
                        raw_data_str = json.dumps(parsed_data, ensure_ascii=False)
                        ai_json = st.session_state.ai_local.enhance_ocr_results(raw_data_str, current_metrics)
                        try:
                            ai_parsed = json.loads(ai_json)
                            if ai_parsed:
                                parsed_data = ai_parsed
                                st.toast("âœ… æœ¬åœ° AI å·²æˆåŠŸæ ¡éªŒå¹¶ä¼˜åŒ–æ•°æ®ç»“æœ", icon="ğŸ¤–")
                        except Exception as e:
                            st.error(f"AI æ ¡éªŒå¤±è´¥: {e}")
                
                if parsed_data:
                    for item in parsed_data: item['category'] = selected_category
                    df = pd.DataFrame(parsed_data)
                    df = df.drop_duplicates(subset=['metric_id', 'period'], keep='first')
                    pivot_df = df.pivot(index='metric_id', columns='period', values='value')
                    labels_map = {m['id']: m['label'] for m in FINANCIAL_METRICS}
                    pivot_df.index = pivot_df.index.map(lambda x: labels_map.get(x, x))
                    st.session_state.parsed_df = pivot_df
                    st.session_state.raw_parsed = parsed_data
                    st.success("è¯†åˆ«å®Œæˆ!")
                else:
                    st.error("è¯†åˆ«å¤±è´¥ï¼Œè¯·æ£€æŸ¥æˆªå›¾ã€‚")

with col_res:
    st.subheader("ğŸ“‹ è¯†åˆ«ç»“æœé¢„è§ˆä¸ç¼–è¾‘")
    if 'parsed_df' in st.session_state:
        edited_df = st.data_editor(st.session_state.parsed_df, use_container_width=True)
        
        target_ticker = st.text_input("å…¬å¸ä»£ç  (Ticker)", value="NVDA")

        if st.button("ğŸ’¾ ç¡®è®¤å¹¶åŒæ­¥åˆ° Wide-Format æ•°æ®åº“"):
            count = 0
            # Reverse map for metric labels to IDs
            metrics_reverse_map = {m['label']: m['id'] for m in FINANCIAL_METRICS}
            
            # Group by Period (Column)
            for period_col in edited_df.columns:
                p_str = str(period_col)
                year_val = 2024
                p_val = p_str
                if "/" in p_str:
                    try:
                        parts = p_str.split("/")
                        year_val = int(parts[0])
                        p_val = parts[1]
                    except: pass
                
                # Prepare wide record dict
                record_dict = {
                    "ticker": target_ticker,
                    "year": year_val,
                    "period": p_val,
                    "category": selected_category,
                    "report_date": st.session_state.auto_disclosure_date
                }
                
                # Add metrics
                for metric_label, row in edited_df.iterrows():
                    m_id = metrics_reverse_map.get(metric_label, metric_label)
                    val = row[period_col]
                    if pd.notna(val) and str(val).strip() != "":
                        clean_val = str(val).replace("äº¿", "").replace("%", "").strip()
                        try: record_dict[m_id] = float(clean_val)
                        except: pass
                
                from shared.schemas.finance import FinancialRecordCreate
                record_in = FinancialRecordCreate(**record_dict)
                st.session_state.repo.create_or_update_record(record_in)
                count += 1
            
            st.success(f"å·²æˆåŠŸåŒæ­¥ {count} ä¸ªæ—¶é—´å‘¨æœŸçš„ Wide-Format æ•°æ®ï¼")
            st.rerun()
    else:
        st.info("å°šæœªè¿›è¡Œè¯†åˆ«ã€‚è¯·å…ˆä¸Šä¼ æˆ–ç²˜è´´æˆªå›¾å¹¶ç‚¹å‡»â€œå¼€å§‹è¯†åˆ«â€ã€‚")

# History View (Wide Format)
st.divider()
st.header("ğŸ“Š æ•°æ®åº“å·²å½•å…¥æ•°æ® (Wide Format)")
all_records = st.session_state.repo.get_all_records()
if all_records:
    history_data = []
    for r in all_records:
        r_dict = {c.name: getattr(r, c.name) for c in r.__table__.columns}
        history_data.append(r_dict)
    
    hist_df = pd.DataFrame(history_data)
    core_cols = ['ticker', 'year', 'period', 'category']
    metric_cols = [c for c in hist_df.columns if c not in core_cols + ['id']]
    # Filter out all-NaN metric columns for cleaner display
    hist_df = hist_df[core_cols + metric_cols].dropna(axis=1, how='all')
    st.dataframe(hist_df, use_container_width=True)
else:
    st.info("æš‚æ— æ•°æ®å½•å…¥è®°å½•ã€‚")
